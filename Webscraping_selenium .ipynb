{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9aa481-5296-4f21-b2fa-27a84cae4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this question you have to scrape data using the filters available on the webpage You have to use the location and salary filter. \n",
    "# You have to scrape data for “Data Scientist” designation for first 10 job results. \n",
    "# You have to scrape the job-title, job-location, company name, experience required. \n",
    "# The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7815a42d-ea24-42f2-a624-acd7915fbcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7704be3-848a-4cc9-a9b7-4f49b1a8f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Set up the Selenium WebDriver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Step 1: Get the web page\n",
    "driver.get(\"https://www.naukri.com\")\n",
    "\n",
    "# Step 2: Enter \"Data Scientist\" in the search field\n",
    "designation = driver.find_element(By.CLASS_NAME, \"qsb\")\n",
    "designation.send_keys(\"Data Scientist\")\n",
    "\n",
    "# Step 3: Apply location\n",
    "# Apply location filter\n",
    "location_filter = driver.find_element(By.XPATH,\"/html/body/div[1]/div[7]/div/div\")\n",
    "location_filter.send_keys(\"Delhi/NCR\")\n",
    "\n",
    "\n",
    "# Step 4: Click the search button\n",
    "search_button = driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search_button.click()\n",
    "\n",
    "# Apply salary filter\n",
    "salary_filter = driver.find_element(By.XPATH, \"//label[contains(text(), '3-6 Lacs')]\")\n",
    "salary_filter.click()\n",
    "\n",
    "# Step 5: Click the search button\n",
    "search_button = driver.find_element(By.CLASS_NAME, \"qsbSubmit\")\n",
    "search_button.click()\n",
    "\n",
    "# Step 6: Scrape the data for the first 10 job results\n",
    "job_listings = []\n",
    "jobs = driver.find_elements(By.CLASS_NAME, 'jobTuple')[:10]\n",
    "\n",
    "for job in jobs:\n",
    "    title = job.find_element(By.CLASS_NAME, '//a[@class=\"title \"]').text.strip()\n",
    "    location = job.find_element(By.CLASS_NAME, //span[@class=\"styles_jhc__location__W_pVs\"]').text.strip()\n",
    "    company = job.find_element(By.CLASS_NAME, 'subTitle').text.strip()\n",
    "    experience = job.find_element(By.CLASS_NAME, 'experience').text.strip()\n",
    "\n",
    "    job_listings.append({\n",
    "        'Job Title': title,\n",
    "        'Job Location': location,\n",
    "        'Company Name': company,\n",
    "        'Experience Required': experience\n",
    "    })\n",
    "\n",
    "# Step 6: Create a DataFrame\n",
    "df_jobs = pd.DataFrame(job_listings)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_jobs)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf502d1-ce81-4f17-8610-be2c72214a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the \n",
    "# job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf12c64-f238-4235-aa83-d554553a5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b3930-6a80-49f4-856b-93de4e92844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Set up the webdriver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "# Step 2: Enter “Data Scientist” in “Job title, Skills” field and enter “Bangalore” in “enter the location” field.\n",
    "job_title_field = driver.find_element(By.ID, \"id_q\")\n",
    "job_title_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "location_field = driver.find_element(By.ID, \"id_loc\")\n",
    "location_field.send_keys(\"Bangalore\")\n",
    "\n",
    "# Step 3: Click the search button\n",
    "search_button = driver.find_element(By.XPATH, \"//button[@class='cls_searchbtn']\")\n",
    "search_button.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 4: Scrape the data for the first 10 jobs results\n",
    "job_titles = []\n",
    "job_locations = []\n",
    "company_names = []\n",
    "experience_required = []\n",
    "\n",
    "jobs = driver.find_elements(By.XPATH, \"//li[@class='search_listing']\")[:10]\n",
    "\n",
    "for job in jobs:\n",
    "    job_title = job.find_element(By.XPATH, \".//h3[@class='job_title']\").text.strip()\n",
    "    job_location = job.find_element(By.XPATH, \".//span[@class='loc']\").text.strip()\n",
    "    company_name = job.find_element(By.XPATH, \".//div[@class='company_name']\").text.strip()\n",
    "    experience = job.find_element(By.XPATH, \".//span[@class='exp']\").text.strip()\n",
    "    \n",
    "    job_titles.append(job_title)\n",
    "    job_locations.append(job_location)\n",
    "    company_names.append(company_name)\n",
    "    experience_required.append(experience)\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "\n",
    "# Step 5: Create a dataframe of the scraped data\n",
    "data = {\n",
    "    'Job Title': job_titles,\n",
    "    'Job Location': job_locations,\n",
    "    'Company Name': company_names,\n",
    "    'Experience Required': experience_required\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f954be5-4df3-491e-bc64-3eb086bac6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "#https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\n",
    "#As shown in the above page you have to scrape the tick marked attributes.These are:\n",
    "#1. Rating\n",
    "#2. Review summary\n",
    "#3. Full review\n",
    "#4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bff18-a5f2-49c4-925e-2110e34300f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9296e988-a231-456d-82c8-803fbcb1be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\") \n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Set up the webdriver\n",
    "service = Service()\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Define the URL\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Open the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Initialize lists to store the data\n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "# Function to scrape reviews from a single page\n",
    "def scrape_reviews():\n",
    "    review_blocks = driver.find_elements(By.CSS_SELECTOR, \"div._1AtVbE\")\n",
    "    for block in review_blocks:\n",
    "        try:\n",
    "            rating = block.find_element(By.CSS_SELECTOR, \"div._3LWZlK\").text\n",
    "            review_summary = block.find_element(By.CSS_SELECTOR, \"p._2-N8zT\").text\n",
    "            full_review = block.find_element(By.CSS_SELECTOR, \"div.t-ZTKy div\").text\n",
    "            ratings.append(rating)\n",
    "            review_summaries.append(review_summary)\n",
    "            full_reviews.append(full_review)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Scrape the first 100 reviews\n",
    "while len(ratings) < 100:\n",
    "    scrape_reviews()\n",
    "    try:\n",
    "        next_button = driver.find_element(By.CSS_SELECTOR, \"a._1LKTO3\")\n",
    "        next_button.click()\n",
    "        time.sleep(2)  # Wait for the next page to load\n",
    "    except:\n",
    "        break\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame with the scraped data\n",
    "reviews_df = pd.DataFrame({\n",
    "    \"Rating\": ratings[:100],\n",
    "    \"Review Summary\": review_summaries[:100],\n",
    "    \"Full Review\": full_reviews[:100]\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "reviews_df.to_csv(\"iphone11_reviews.csv\", index=False)\n",
    "\n",
    "print(\"Scraping completed and data saved to iphone11_reviews.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea378248-2af9-4eb7-bb2c-dad119066313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” inthe search field.\n",
    "# You have to scrape 3 attributes of each sneaker:\n",
    "# 1.Brand\n",
    "# 2.Product bDescription\n",
    "# 3.Price\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b11f026-36f3-4d55-bd61-9efb9b137699",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c0ef79-1fe4-4f06-9da0-908865351961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver \n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Flipkart\n",
    "driver.get('https://www.flipkart.com')\n",
    "\n",
    "# Close the login pop-up if it appears\n",
    "try:\n",
    "    close_button = driver.find_element(By.XPATH, \"//button[contains(text(), '✕')]\")\n",
    "    close_button.click()\n",
    "except Exception as e:\n",
    "    print(\"Login pop-up not found.\")\n",
    "\n",
    "# Search for \"sneakers\"\n",
    "search_box = driver.find_element(By.NAME, 'q')\n",
    "search_box.send_keys('sneakers')\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Scrape sneaker data\n",
    "sneakers_data = []\n",
    "\n",
    "# Loop through the first 100 sneakers\n",
    "for i in range(1, 11):  # Adjust the range based on pages\n",
    "    sneakers = driver.find_elements(By.XPATH, \"//div[contains(@class, '_1AtVbE')]//div[contains(@class, '_2kHMtA')]\")\n",
    "    \n",
    "    for sneaker in sneakers:\n",
    "        try:\n",
    "            brand = sneaker.find_element(By.XPATH, \".//div[contains(@class, '_2B099V')]\").text\n",
    "            description = sneaker.find_element(By.XPATH, \".//a[contains(@class, 'IRpwTa')]\").text\n",
    "            price = sneaker.find_element(By.XPATH, \".//div[contains(@class, '_30jeq3')]\").text\n",
    "            \n",
    "            sneakers_data.append({\n",
    "                'Brand': brand,\n",
    "                'Description': description,\n",
    "                'Price': price\n",
    "            })\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # Go to the next page\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[contains(@class, '_1LKTO3')]\")\n",
    "        next_button.click()\n",
    "        time.sleep(3)\n",
    "    except Exception as e:\n",
    "        print(\"No more pages or error encountered.\")\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Output the data\n",
    "for sneaker in sneakers_data[:100]:  # Display only first 100\n",
    "    print(sneaker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b2c57-e813-41cc-a913-7db13574c060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU Type filter to “Intel Core i7” as shown in the below image\n",
    "# After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "# 1.Title\n",
    "# 2.Ratings\n",
    "# 3.Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e9df1-4d7d-4e01-9744-c261d49f4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c979f3-ae00-4564-a87d-14af3cd64e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open Amazon\n",
    "driver.get('https://www.amazon.in/')\n",
    "\n",
    "# Search for \"Laptop\"\n",
    "search_box = driver.find_element(By.ID, 'twotabsearchtextbox')\n",
    "search_box.send_keys('Laptop')\n",
    "search_box.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Set CPU Type filter to \"Intel Core i7\"\n",
    "# You may need to adjust this XPATH based on the page structure\n",
    "try:\n",
    "    filter_element = driver.find_element(By.XPATH, \"//span[contains(text(), 'Intel Core i7')]\")\n",
    "    filter_element.click()\n",
    "    time.sleep(3)  # Wait for the filter to apply\n",
    "except Exception as e:\n",
    "    print(\"Filter not found or could not be clicked.\")\n",
    "\n",
    "# Scrape laptop data\n",
    "laptops_data = []\n",
    "\n",
    "# Locate the laptops on the page\n",
    "laptops = driver.find_elements(By.XPATH, \"//div[@data-component-type='s-search-result']\")[:10]\n",
    "\n",
    "for laptop in laptops:\n",
    "    try:\n",
    "        title = laptop.find_element(By.XPATH, \".//span[@class='a-size-medium a-color-base a-text-normal']\").text\n",
    "        rating = laptop.find_element(By.XPATH, \".//span[@class='a-icon-alt']\").text\n",
    "        price = laptop.find_element(By.XPATH, \".//span[@class='a-price-whole']\").text\n",
    "        \n",
    "        # Add to list\n",
    "        laptops_data.append({\n",
    "            'Title': title,\n",
    "            'Ratings': rating,\n",
    "            'Price': price\n",
    "        })\n",
    "    except Exception as e:\n",
    "        continue  # Skip if any information is missing\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Output the scraped data\n",
    "for laptop in laptops_data:\n",
    "    print(laptop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f508e-143b-423f-97ea-c54457b3fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Write a python program to scrape data for Top 1000 Quotes of All Time.The above task will be done in following steps:\n",
    "# 1.First get the webpagehttps://www.azquotes.com/\n",
    "# 2.Click on TopQuote\n",
    "# 3.Than scrap a)Quote b) Author c) Type Of Quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd45f68-8c8d-4e6c-9817-889f60955d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae2dca-2c5f-45a7-adf0-141e586efdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Set up the WebDriver (adjust the path to your WebDriver)\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Step 1: Open the AZQuotes website\n",
    "driver.get('https://www.azquotes.com/')\n",
    "\n",
    "# Step 2: Click on \"Top Quotes\"\n",
    "top_quotes_link = driver.find_element(By.LINK_TEXT,'Top quotes')\n",
    "top_quotes_link.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 3: Scrape data\n",
    "quotes_data = []\n",
    "\n",
    "# Loop through pages to get top 1000 quotes\n",
    "while len(quotes_data) < 1000:\n",
    "    # Get all quotes on the current page\n",
    "    quotes = driver.find_elements(By.XPATH, \"//div[@class='quote']\")\n",
    "\n",
    "    for quote in quotes:\n",
    "        if len(quotes_data) >= 1000:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            quote_text = quote.find_element(By.XPATH, \".//a[@class='title']\").text\n",
    "            author = quote.find_element(By.XPATH, \".//a[@class='author']\").text\n",
    "            quote_type = quote.find_element(By.XPATH, \".//div[@class='tags']\").text\n",
    "            \n",
    "            quotes_data.append({\n",
    "                'Quote': quote_text,\n",
    "                'Author': author,\n",
    "                'Type of Quote': quote_type\n",
    "            })\n",
    "        except Exception as e:\n",
    "            continue  # Skip if any information is missing\n",
    "\n",
    "    # Check for the \"Next\" button\n",
    "    try:\n",
    "        next_button = driver.find_element(By.LINK_TEXT, 'Next')\n",
    "        next_button.click()\n",
    "        time.sleep(3)  # Wait for the next page to load\n",
    "    except Exception as e:\n",
    "        print(\"No more pages or error encountered.\")\n",
    "        break\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Output the first 10 quotes to check the results\n",
    "for quote in quotes_data[:10]:\n",
    "    print(quote)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8e80e-6c24-44b3-9862-b371c36a30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to display list of respected former Prime Ministers of India (i.e. Name,Born-Dead, Term of office, Remarks)\n",
    "# from https://www.jagranjosh.com/general-knowledge/list-of\u0002all-prime-ministers-of-india-1473165149-1\n",
    "# scrap the mentioned data and make the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99805a9f-2b1f-4746-957a-6d4ddfab8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e320055-e6b6-41ac-9601-92d74a5f2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the webdriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "driver.get(\"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\")\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 2: Scrape the data and make the dataframe\n",
    "names = []\n",
    "born_dead = []\n",
    "terms_of_office = []\n",
    "remarks = []\n",
    "\n",
    "# Locate the table containing the data\n",
    "table = driver.find_element(By.XPATH, '//*[@id=\"article\"]/div[2]/div[1]/table')\n",
    "\n",
    "# Iterate over each row in the table (excluding the header row)\n",
    "for row in table.find_elements(By.XPATH, './/tbody/tr'):\n",
    "    cells = row.find_elements(By.TAG_NAME, 'td')\n",
    "    if len(cells) == 4:  # Ensure there are exactly 4 cells\n",
    "        names.append(cells[0].text.strip())  # Strip any extra whitespace\n",
    "        born_dead.append(cells[1].text.strip())\n",
    "        terms_of_office.append(cells[2].text.strip())\n",
    "        remarks.append(cells[3].text.strip())\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": names,\n",
    "    \"Born-Dead\": born_dead,\n",
    "    \"Term of Office\": terms_of_office,\n",
    "    \"Remarks\": remarks\n",
    "})\n",
    "\n",
    "# Close the webdriver\n",
    "driver.quit()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d164e85-34cb-4114-b565-b5a4226aa2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/\n",
    "# This task will be done in following steps:\n",
    "# 1. First get the webpage https://www.motor1.com/\n",
    "# 2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "# 3. Then click on 50 most expensive carsin the world..\n",
    "# 4. Then scrap thementioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5185f6-3b40-47ae-9dc5-66ec04a52cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e0f2bc-541a-425b-82ea-70765d7dd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize the WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "# Step 1: Get the webpage\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "\n",
    "# Step 2: Type in the search bar '50 most expensive cars'\n",
    "search_bar = driver.find_element(By.NAME, \"query\")\n",
    "search_bar.send_keys(\"50 most expensive cars\")\n",
    "search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "# Wait for search results to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Step 3: Click on '50 Most Expensive Cars'\n",
    "link = driver.find_element(By.PARTIAL_LINK_TEXT, \"50 Most Expensive Cars\")\n",
    "link.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)\n",
    "\n",
    "# Step 4: Scrape the data (Car name and Price)\n",
    "car_names = driver.find_elements(By.XPATH, \"//h2[contains(@class, 'title')]\")\n",
    "car_prices = driver.find_elements(By.XPATH, \"//span[contains(@class, 'price')]\")\n",
    "\n",
    "# Extract text from elements and store in lists\n",
    "names = [car.text for car in car_names]\n",
    "prices = [price.text for price in car_prices]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    \"Car Name\": names,\n",
    "    \"Price\": prices\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199a70f-43cb-4b23-9ef5-2bb68d552376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
