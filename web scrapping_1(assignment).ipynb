{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2edeffe2-e6b1-4ebf-9782-26cca20574c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Rating, Year]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to display IMDB’s Top rated 100 Indian movies’ data and make data frame\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the IMDb list\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "\n",
    "# Fetch the content from the URL\n",
    "response = requests.get('https://www.imdb.com/list/ls056092300/')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "names = []\n",
    "ratings = []\n",
    "years = []\n",
    "\n",
    "# Find all movie containers\n",
    "movie_containers = soup.find_all('span', class_='sc-1f50b7c-0 iPPbjm\"')\n",
    "\n",
    "for container in movie_containers:\n",
    "    # Movie name\n",
    "    name = container.h3.a.text\n",
    "    \n",
    "    # Movie rating\n",
    "    rating = container.find('span', class_='sc-40b53d-1 kJANdR').text\n",
    "    \n",
    "    # Movie year of release\n",
    "    year = container.h3.find('a', class_='ipc-link ipc-link--baseAlt ipc-link--inherit-color').text.strip('()')\n",
    "    \n",
    "    names.append(name)\n",
    "    ratings.append(rating)\n",
    "    years.append(year)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Name': names,\n",
    "    'Rating': ratings,\n",
    "    'Year': years\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv('top_100_indian_movies.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f471173-ddca-43d0-9e9b-a3b0aebac924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Heading, Date, Content, Likes, YouTube Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape details of all the posts.\n",
    "#Scrape theheading, date, content and the likes for the video from the link for the youtube video from the post\n",
    "# create data frame\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL of the Patreon page\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "\n",
    "# Fetch the content from the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "headings = []\n",
    "dates = []\n",
    "contents = []\n",
    "likes = []\n",
    "youtube_links = []\n",
    "\n",
    "# Find all post containers\n",
    "post_containers = soup.find_all('article', class_='Post')\n",
    "\n",
    "for post in post_containers:\n",
    "    # Post heading\n",
    "    heading = post.find('span', class_=\"post-title\").text.strip()\n",
    "    \n",
    "    # Post date\n",
    "    date = post.find('span', class_='sc-dkPtRN jHGRRz').text.strip()\n",
    "    \n",
    "    # Post content\n",
    "    content = post.find('div', class_='sc-1cq1psq-0 kWkNqn sc-2ohj6g-0 jBUmLB').text.strip()\n",
    "    \n",
    "    # Post likes\n",
    "    likes_tag = post.find('svg', class_='IconHeart')\n",
    "    likes = likes_tag.text.strip() if likes_tag else 'N/A'\n",
    "    \n",
    "    # YouTube video link\n",
    "    youtube_link = None\n",
    "    video_iframe = post.find('iframe', src=True)\n",
    "    if video_iframe and 'youtube.com' in video_iframe['src']:\n",
    "        youtube_link = video_iframe['src']\n",
    "    \n",
    "    # Append data to lists\n",
    "    headings.append(heading)\n",
    "    dates.append(date)\n",
    "    contents.append(content)\n",
    "    likes.append(likes)\n",
    "    youtube_links.append(youtube_link)\n",
    "\n",
    "# Create a DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Heading': headings,\n",
    "    'Date': dates,\n",
    "    'Content': contents,\n",
    "    'Likes': likes,\n",
    "    'YouTube Link': youtube_links\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv('patreon_posts.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf1491e9-cd0a-493a-9cfd-2c5d2ab5b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Location, Area, EMI, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape house details from mentioned URL. \n",
    "#It should include house title, location,area, EMI and price from Enter three localities which are Indira Nagar, Jayanagar,Rajaji Nagar.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data from a locality\n",
    "def scrape_locality(locality):\n",
    "    # URL of the NoBroker search page for a specific locality\n",
    "    url = f'https://www.nobroker.in/property/search/{locality}'\n",
    "    \n",
    "    # Fetch the content from the URL\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    titles = []\n",
    "    locations = []\n",
    "    areas = []\n",
    "    emis = []\n",
    "    prices = []\n",
    "    \n",
    "    # Find all house containers\n",
    "    house_containers = soup.find_all('div', class_='nb__2lQH')\n",
    "    \n",
    "    for house in house_containers:\n",
    "        # House title\n",
    "        title = house.find('span', class_='property-search-label text-white').text.strip() if house.find('div', class_='property-search-label text-white') else 'N/A'\n",
    "        \n",
    "        # House location\n",
    "        location = house.find('div', class_='nb__1l5t').text.strip() if house.find('div', class_='nb__1l5t') else 'N/A'\n",
    "        \n",
    "        # House area\n",
    "        area = house.find('div', class_='nb__2JzD').text.strip() if house.find('div', class_='nb__2JzD') else 'N/A'\n",
    "        \n",
    "        # EMI\n",
    "        emi = house.find('div', class_='nb__3L0k').text.strip() if house.find('div', class_='nb__3L0k') else 'N/A'\n",
    "        \n",
    "        # House price\n",
    "        price = house.find('div', class_='nb__3Twx').text.strip() if house.find('div', class_='nb__3Twx') else 'N/A'\n",
    "        \n",
    "        # Append data to lists\n",
    "        titles.append(title)\n",
    "        locations.append(location)\n",
    "        areas.append(area)\n",
    "        emis.append(emi)\n",
    "        prices.append(price)\n",
    "    \n",
    "    return titles, locations, areas, emis, prices\n",
    "\n",
    "# Localities to scrape\n",
    "localities = ['indira-nagar', 'jayanagar', 'rajaji-nagar']\n",
    "\n",
    "# Lists to hold all data\n",
    "all_titles = []\n",
    "all_locations = []\n",
    "all_areas = []\n",
    "all_emis = []\n",
    "all_prices = []\n",
    "\n",
    "# Scrape data for each locality\n",
    "for locality in localities:\n",
    "    titles, locations, areas, emis, prices = scrape_locality(locality)\n",
    "    all_titles.extend(titles)\n",
    "    all_locations.extend(locations)\n",
    "    all_areas.extend(areas)\n",
    "    all_emis.extend(emis)\n",
    "    all_prices.extend(prices)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'Title': all_titles,\n",
    "    'Location': all_locations,\n",
    "    'Area': all_areas,\n",
    "    'EMI': all_emis,\n",
    "    'Price': all_prices\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "df.to_csv('house_details.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "42fefc17-c8da-4814-a9e4-419db6387c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python program to scrape first 10 product details which include product name , price , Image from\n",
    "# URL-https://www.bewakoof.com/bestseller?sort=popular.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage to scrape\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all product containers (this might change based on the webpage structure)\n",
    "    products = soup.find_all('div', class_='product-card', limit=10)\n",
    "\n",
    "    # Extract and print details for each product\n",
    "    for product in products:\n",
    "        # Extract product name\n",
    "        name = product.find('h3', class_='testProManufacturer').get_text(strip=True)\n",
    "        \n",
    "        # Extract product price\n",
    "        price = product.find('div', class_='discountedPriceText clr-p-black false').get_text(strip=True)\n",
    "        \n",
    "        # Extract image URL\n",
    "        image_tag = product.find('img', class_='https://images.bewakoof.com/t640/men-s-black-adam-graphic-printed-t-shirt-541266-1709214736-1.jpg')\n",
    "        image_url = image_tag['src'] if image_tag else 'No image URL'\n",
    "        \n",
    "        # Print the details\n",
    "        print(f'Product Name: {name}')\n",
    "        print(f'Price: {price}')\n",
    "        print(f'Image URL: {image_url}')\n",
    "        print('---')\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c8604e2-a44b-44ee-af45-4bac6b6e0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please visit https://www.cnbc.com/world/?region=world and scrap-\n",
    " #a) headings\n",
    " #b) date\n",
    " #c) News link\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the CNBC world news page\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all news articles (the specific classes might need adjustment based on the page structure)\n",
    "    articles = soup.find_all('div', class_='Card-standardBreaker', limit=10)\n",
    "\n",
    "    # Extract and print details for each article\n",
    "    for article in articles:\n",
    "        # Extract heading\n",
    "        heading = article.find('h1', class_='ArticleHeader-headline').get_text(strip=True)\n",
    "        \n",
    "        # Extract date (this might be complex if the date is not directly available in the same container)\n",
    "        # For this example, we assume that date information might be within the article or another tag\n",
    "        date_tag = article.find('published-timestamp')\n",
    "        date = date_tag.get_text(strip=True) if date_tag else 'No date available'\n",
    "        \n",
    "        # Extract news link\n",
    "        link_tag = article.find('a href', class_=\"https://apnews.com/hub/israel-hamas-war\")\n",
    "        news_link = link_tag['href'] if link_tag else 'No link available'\n",
    "        \n",
    "        # Print the details\n",
    "        print(f'Heading: {heading}')\n",
    "        print(f'Date: {date}')\n",
    "        print(f'News Link: {news_link}')\n",
    "        print('---')\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1372abc3-361f-4746-85f3-3eea8fe0cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the webpage. Status code: 404\n"
     ]
    }
   ],
   "source": [
    "# Please visit https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/ and scrapa) \n",
    " # a)Paper title\n",
    " # b) date\n",
    " # c) Author\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the KeAi Publishing page with most downloaded articles\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloadedarticles/'\n",
    "\n",
    "# Send an HTTP request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all article containers (the specific classes might need adjustment based on the page structure)\n",
    "    articles = soup.find_all('div', class_='article-item')  # Adjust class name if needed\n",
    "\n",
    "    # Extract and print details for each article\n",
    "    for article in articles:\n",
    "        # Extract paper title\n",
    "        title_tag = article.find('h1', class_='Artificial Intelligence in Agriculture')  # Adjust class name if needed\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'No title available'\n",
    "        \n",
    "        # Extract date\n",
    "        date_tag = article.find('div', class_='mb3')  # Adjust class name if needed\n",
    "        date = date_tag.get_text(strip=True) if date_tag else 'No date available'\n",
    "        \n",
    "        # Extract author\n",
    "        author_tag = article.find('h1', class_='Guide for Authors')  # Adjust class name if needed\n",
    "        author = author_tag.get_text(strip=True) if author_tag else 'No author available'\n",
    "        \n",
    "        # Print the details\n",
    "        print(f'Paper Title: {title}')\n",
    "        print(f'Date: {date}')\n",
    "        print(f'Author(s): {author}')\n",
    "        print('---')\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cbb574-06d9-4ccc-8771-c70117168736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
